# 微分

## **偏导数（partial derivative）**

> 多元函数的微分
> 

## 梯度

> 多元函数所有变量的（偏导数），就是它的gradient vector（梯度向量）
> 

![image.png](%E5%BE%AE%E5%88%86%20162a4fd6ce50801ab6abff37c018910b/image.png)

![47979e2538058e1e7926a6b3c06b536.jpg](%E5%BE%AE%E5%88%86%20162a4fd6ce50801ab6abff37c018910b/47979e2538058e1e7926a6b3c06b536.jpg)

## 自动微分（automatic differentiation）

- 求导 y = 2x^T x
    
    ```python
    import torch
    x = torch.arange(4.0)
    x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)
    x.grad  # 默认值是None
    y = 2 * torch.dot(x, x)
    y.backward()
    x.grad
    #tensor([ 0.,  4.,  8., 12.])
    
    # 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
    # 本例只想求偏导数的和，所以传递一个1的梯度是合适的
    x.grad.zero_()
    y = x * x
    # 等价于y.backward(torch.ones(len(x)))
    y.sum().backward()
    x.grad
    ```
    

正向 vs 反向：

生成运算图（no cycle），

前向：执行图，储存中间结果

反向：反方向执行图，剪支

复杂度：计算都是O（n），反向的内存O（1）

## !!!!

$$
{\partial (x_1^2+2x^2_2) \over \partial x}=[2x_1,4x_2]
$$

点（1，1），偏导数(梯度) = （2,4）

梯度的方向与等高线度正交，意味着梯度指向这个值变化最大的方向

![image.png](%E5%BE%AE%E5%88%86%20162a4fd6ce50801ab6abff37c018910b/image%201.png)