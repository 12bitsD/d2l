# 基础优化方法

## 梯度下降：

挑选一个初始值(随机)，w0

迭代法接近0

$$
w_t=w_t-1 - \eta({\partial l \over \partial w_{t-1}}) \newline \eta（hyper\ parameter）;沿着梯度的反方向（-gradient）走多远
$$

### 小批量随机梯度下降

> 整个训练集上算梯度太贵
> 

随机取样b哥样本，进行近似损失

$$
{1 \over b}\sum_{i \in I_b}l(x_i,y_i,w) \newline b个样本,i_1,i_2,i_3...去随机做梯度下降，取均值
$$

b-批量 batch size （不能太大，不能太小）

梯度下降通过不断沿着梯度反方向 ，使用迭代法求解

小批量随机梯度下降，是深度学习默认的求解方式

两个重要的超参数：批量大小-b，学习率-eta