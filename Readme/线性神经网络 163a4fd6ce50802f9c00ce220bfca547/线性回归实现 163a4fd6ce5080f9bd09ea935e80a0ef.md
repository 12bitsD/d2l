# 线性回归实现

### 1. 生成合成数据

```python
def synthetic_data(w, b, num_examples):
    """生成x=Xw+b+噪音"""
    x = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(x, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return x, y.reshape((-1, 1))  # 转换成column 向量方便后面算梯度

t_w = torch.tensor([2, -3.4])
t_b = 4.2
features, labels = synthetic_data(t_w, t_b, 1000)

```

- **`synthetic_data`函数**：生成合成的数据集。
    - **输入参数**：
        - `w`：权重向量。
        - `b`：偏置。
        - `num_examples`：生成的样本数量。
    - **生成过程**：
        - `x`：从标准正态分布中随机生成`num_examples`行、`len(w)`列的矩阵。
        - `y`：计算`y = Xw + b`，并添加均值为0、标准差为0.01的噪声。
        - `y.reshape((-1, 1))`：将`y`转换为列向量，方便后续计算梯度。
- **`t_w`和`t_b`**：定义真实的权重和偏置。
- **`features, labels`**：调用`synthetic_data`生成1000个样本的特征和标签。

### 2. 数据迭代器

```python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # 打乱indices, 随机读取
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        # 取出indicies里面的batch_size个元素，如果溢出了，选择num_examples
        batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]

batch_size = 10
for X, y in data_iter(batch_size, features, labels):
    print(X, '\\n', y)
    break

```

- **`data_iter`函数**：生成一个数据迭代器，用于随机批量读取数据。
    - **输入参数**：
        - `batch_size`：每个批次的大小。
        - `features`：特征矩阵。
        - `labels`：标签向量。
    - **过程**：
        - 生成从0到`num_examples-1`的索引列表`indices`。
        - 打乱`indices`，以实现随机读取。
        - 按`batch_size`分批取出索引，生成`batch_indices`。
        - 使用`yield`返回当前批次的特征和标签。
- **`batch_size`**：定义每个批次的大小为10。
- **`for X, y in data_iter(...)`**：遍历数据迭代器，打印第一个批次的特征和标签。

### 3. 模型参数初始化

```python
# 模型参数
w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

```

- **`w`**：初始化权重向量，从均值为0、标准差为0.01的正态分布中随机生成，形状为`(2, 1)`，并设置`requires_grad=True`以便自动计算梯度。
- **`b`**：初始化偏置为0，形状为`(1,)`，并设置`requires_grad=True`。

### 4. 线性回归模型

```python
# 模型参数
def linreg(X, w, b):
    return torch.matmul(X, w) + b
```

- **`linreg`函数**：定义线性回归模型。
    - **输入参数**：
        - `X`：特征矩阵。
        - `w`：权重向量。
        - `b`：偏置。
    - **返回值**：计算`Xw + b`，即线性回归模型的输出。

### 5. 损失函数

```python
# 损失函数-均方误差
def squared_loss(y_hat, y):
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2

```

- **`squared_loss`函数**：定义均方误差损失函数。
    - **输入参数**：
        - `y_hat`：模型的预测值。
        - `y`：真实标签。
    - **返回值**：计算`(y_hat - y)^2 / 2`，即均方误差。

### 6. 优化算法

```python
# 优化算法，sgd 小规模随机梯度下降
def sgd(params, lr, batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

- **`sgd`函数**：实现小批量随机梯度下降算法。
    - **输入参数**：
        - `params`：模型参数列表，即`[w, b]`。
        - `lr`：学习率。
        - `batch_size`：批次大小。
    - **过程**：
        - 在`torch.no_grad()`上下文中，避免计算图的构建。
        - 对每个参数`param`，更新其值：`param -= lr * param.grad / batch_size`。
        - 更新后，将梯度清零：`param.grad.zero_()`。

### 7. 训练过程

```python
lr = 0.03
num__epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num__epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # 计算x, y的小批量损失
        # l的形状是，[batch_size, 1]而非标量，需要将l中的所有元素加在一起，来计算[w, b]的梯度
        l.sum().backward()
        sgd([w, b], lr, batch_size)

    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

- **`lr`**：学习率设置为0.03。
- **`num__epochs`**：训练轮数设置为3。
- **`net`**：定义模型为`linreg`。
- **`loss`**：定义损失函数为`squared_loss`。
- **训练过程**：
    - 外层循环遍历每个epoch。
    - 内层循环遍历每个批次的数据`X, y`。
        - 计算当前批次的损失`l`。
        - 对损失`l`求和，并调用`backward()`计算梯度。
        - 调用`sgd`更新模型参数`w`和`b`。
    - 每个epoch结束后，计算并打印整个训练集的平均损失。

### 总结

- **数据生成**：通过`synthetic_data`函数生成合成的线性回归数据。
- **数据迭代器**：通过`data_iter`函数实现随机批量读取数据。
- **模型定义**：通过`linreg`函数定义线性回归模型。
- **损失函数**：通过`squared_loss`函数定义均方误差损失。
- **优化算法**：通过`sgd`函数实现小批量随机梯度下降。
- **训练过程**：通过两层循环实现模型的训练，并在每个epoch结束后打印训练集的平均损失。